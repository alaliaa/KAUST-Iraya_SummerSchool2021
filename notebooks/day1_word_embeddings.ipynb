{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UTILISING UNSTRUCTURED DATA IN GEOSCIENCE\n",
    "## DAY ONE: WORD EMBEDDINGS\n",
    "\n",
    "This tutorial is split into three parts:\n",
    "1. Investigeting pre-trained embeddings looking at finding most similar words and computing word-vector maths\n",
    "2. Creating our own embeddings\n",
    "3. Comparing our embeddings with the pre-trained embeddings\n",
    "    \n",
    "Prior to beginning, please ensure you have downloaded the data and installed the python packages detailed in the environment.yaml file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODAY'S CHALLENGE\n",
    "\n",
    "The challenge for day one is: \n",
    "\n",
    "   **Create a set of geoscientific word embeddings and identify the most similar term to 5 given terms. Similarly, calculate the nearest term to a vector maths problem**\n",
    "   \n",
    "Terms for the most similar:\n",
    "- salt\n",
    "- ghost\n",
    "- gather\n",
    "- elastic\n",
    "\n",
    "Vector calculations to compute:\n",
    "- P-wave - compressional + shear\n",
    "- seal - mudstone + sandstone\n",
    "- PSTM - time + depth\n",
    "- Kirchoff - ray + wavefield\n",
    "    \n",
    "Please submit all results via https://forms.gle/RPjt4af7smMToq4z8 by 11:59pm GMT on 15 June 2021. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook set-up\n",
    "Importing all the packages that we will need for this tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/cebirnie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imported packages include:\n",
    "\n",
    "- **gensim** :  https://pypi.org/project/gensim/\n",
    "\n",
    "    This is the primary package we will be using for generating and using the word embeddings.\n",
    "    \n",
    "    \n",
    "- **nltk** : https://www.nltk.org/\n",
    "\n",
    "    This is the package we will use for processing of the text data prior to its conversion to word embeddings. The 'punkt' extension is to provide sentence tokenisation. (Tokenisation will be discussed below.)\n",
    "    \n",
    "    \n",
    "- **pandas** : https://pandas.pydata.org/\n",
    "\n",
    "    We will use this package to aid our comparison between the pre-trained and custom-made embeddings, as well as to load in our corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 1: Getting to grips with pre-trained word embeddings\n",
    "\n",
    "In this section we are going to use pre-trained word embeddings, look at word similarity and perform some vector maths.\n",
    "\n",
    "The word vectors have been downloaded from: http://vectors.nlpl.eu/repository/ a resource by the University of Oslo where you will find many more pretrained word embeddings.\n",
    "\n",
    "The downloaded embeddings are loaded using the Gensim Python package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the pre-trained vectors from file, I assume they have been placed into a folder called data that is one level up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "data_dir = \"../data/\"\n",
    "\n",
    "# Load vectors \n",
    "wikiemb_path = os.path.join(data_dir, \"wiki_w2v.bin\")\n",
    "wiki_vecs = gensim.models.KeyedVectors.load_word2vec_format(gpath, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Similarity\n",
    "\n",
    "Using the cosine distance we can compute the similarity between neighbouring words.\n",
    "\n",
    "We are going to use the function: https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the documentation of the function\n",
    "wiki_vecs.most_similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: How to find 10 most similar word to `happy`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dataset', 0.08874545991420746),\n",
       " ('pipelines', 0.08865741640329361),\n",
       " ('Examination', 0.0821446105837822),\n",
       " ('Encana', 0.07870388776063919),\n",
       " ('NUC', 0.077957883477211),\n",
       " ('PDR', 0.07451966404914856),\n",
       " ('Datum', 0.0742947906255722),\n",
       " ('Quadro', 0.07304269820451736),\n",
       " ('Seismic', 0.07278499007225037),\n",
       " ('NMC', 0.07264432311058044)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_vecs.most_similar(positive=['happy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do:**\n",
    "1. Find the 10 most similar words to `education`\n",
    "2. Find the most similar word to `science`\n",
    "3. Advanced: Find the least similar word to `happy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic word vector maths\n",
    "\n",
    "Similar to how we identified the most similar words to a single word, as the words are in a vector format then we can combine the vectors and find the most similar word to the combination of the vectors. Such vector summations equal the equivalent of analogies in the language domain. \n",
    "\n",
    "Again, here we shall be using word2vec's `most_similar` function, but this time instead of passing a single word as either positive or negative, we will pass in a list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking back at the documentation notice that the input to 'positive' and 'negative' are lists\n",
    "wiki_vecs.most_similar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: What is the female equivalent of a king?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('queen', 0.7168769240379333)\n"
     ]
    }
   ],
   "source": [
    "print (wiki_vecs.most_similar(positive=[\"king\", \"woman\"], negative = [\"man\"])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: To understand which vectors should be summed versus subtracted consider what the analogy is. In this scenario, we start with the positive king vector and we wish to remove the male vector and add the female. As such the equation would be: **king-man+woman**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2: What is the capital of England?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('London', 0.6139545440673828)\n"
     ]
    }
   ],
   "source": [
    "print (wiki_vecs.most_similar(positive=[\"Oslo\", \"England\"], negative = [\"Norway\"])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation: In this example we were not told which analogy to use therefore we had to understand the the question is asking about a country and its capital city and use this to make our own analogy. In this case we used the equation: **Oslo-Norway+England**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do:**\n",
    "1. Repeat example 2 with your own analogy to find the capital of Scotland\n",
    "2. Write your own analogy and equation to compute the past tense of run\n",
    "3. Write your own analogy and equation to determine the colour of the sky\n",
    "4. Advanced: We do not always need 3 components to the equation. Answer the following: What is a king if he is not a royal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Creating our own word embeddings\n",
    "\n",
    "In this section we are going to generate our own word embeddings from geoscientific texts. \n",
    "\n",
    "To do so we will need to: \n",
    "1. read in our corpus (geoscientific text),\n",
    "2. perform any necessary processing of the corpus,\n",
    "3. compute the word vectors\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ORIGINAL: Hopefully \n",
    "text_filepath = os.path.join(data_dir, \"geoscience_corpus.txt\")\n",
    "\n",
    "# Read in text line by line\n",
    "with open(text_filepath) as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "# remove whitespace characters like `\\n` at the end of each line\n",
    "content = [x.strip() for x in content] \n",
    "\n",
    "# take a quick look at what has been read in\n",
    "content[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data loading\n",
    "Iraya Energies has very kindly provided the corpus for this summer school. The corpus is composed of summaries of geoscience conference abstracts and journal papers. \n",
    "\n",
    "If using the flat file, I assume that this is in the same location as the wiki embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>page_num</th>\n",
       "      <th>par_num</th>\n",
       "      <th>is_cli</th>\n",
       "      <th>file</th>\n",
       "      <th>doc_text</th>\n",
       "      <th>remarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>d4018029-1b2f-45ca-a9d1-ebacfa337567</td>\n",
       "      <td>Yogyakarta is one of most-populated provinces ...</td>\n",
       "      <td>{'TITLE': 'Determining Groundwater Recharge Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>f90fb8fe-225a-404c-b0d2-78c9e6586aa2</td>\n",
       "      <td>The magnetotelluric (MT) 1D inversion modeling...</td>\n",
       "      <td>{'TITLE': 'Magnetotelluric 1D Inversion Using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>215e638f-328d-4c01-a3b2-971dbea28321</td>\n",
       "      <td>In direct current (DC) sounding (VES) data, in...</td>\n",
       "      <td>{'TITLE': 'Comparison of Particle Swarm Optimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>14b0e724-a9f9-4258-b908-8a91912f1978</td>\n",
       "      <td>In order to perform seismic inversion and have...</td>\n",
       "      <td>{'TITLE': 'Quantification of errors in well-tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>36d262a1-4bce-4571-8e85-9b090f7b019f</td>\n",
       "      <td>Seismic wave energy attenuation and velocity d...</td>\n",
       "      <td>{'TITLE': 'Reflectivity Dispersion for Gas Det...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classification  page_num  par_num  is_cli  \\\n",
       "0           text         1        1   False   \n",
       "1           text         1        1   False   \n",
       "2           text         1        1   False   \n",
       "3           text         1        1   False   \n",
       "4           text         1        1   False   \n",
       "\n",
       "                                   file  \\\n",
       "0  d4018029-1b2f-45ca-a9d1-ebacfa337567   \n",
       "1  f90fb8fe-225a-404c-b0d2-78c9e6586aa2   \n",
       "2  215e638f-328d-4c01-a3b2-971dbea28321   \n",
       "3  14b0e724-a9f9-4258-b908-8a91912f1978   \n",
       "4  36d262a1-4bce-4571-8e85-9b090f7b019f   \n",
       "\n",
       "                                            doc_text  \\\n",
       "0  Yogyakarta is one of most-populated provinces ...   \n",
       "1  The magnetotelluric (MT) 1D inversion modeling...   \n",
       "2  In direct current (DC) sounding (VES) data, in...   \n",
       "3  In order to perform seismic inversion and have...   \n",
       "4  Seismic wave energy attenuation and velocity d...   \n",
       "\n",
       "                                             remarks  \n",
       "0  {'TITLE': 'Determining Groundwater Recharge Po...  \n",
       "1  {'TITLE': 'Magnetotelluric 1D Inversion Using ...  \n",
       "2  {'TITLE': 'Comparison of Particle Swarm Optimi...  \n",
       "3  {'TITLE': 'Quantification of errors in well-tr...  \n",
       "4  {'TITLE': 'Reflectivity Dispersion for Gas Det...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geodata = pd.read_json(os.path.join(data_dir, \"document_info.json\"))\n",
    "geodata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we are only interested in the summaries of the documents so let us just extract out that information from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotexts = geodata['doc_text'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of summaries. First, lets check how many summaries we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1047"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(geotexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And lets look at the first few summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yogyakarta is one of most-populated provinces in Indonesia having a high groundwater utilization. Most of water usages are still derived from groundwater resource. This condition is exacerbated by urbanization process which has a big deal on decreasing recharge area of groundwater due to land use change. To understand the water balance and the vulnerability of groundwater in Yogyakarta, the recharge area of groundwater need to be analyzed. The quantifying of recharge potential zone in Yogyakarta, was conducted by the integration of all factors influencing the hydrogeological process, those are lithology, land cover/land use, lineament and drainage frequency density, and geomorphology. The data were gained from satellite images (DEM and Landsat 8) and other exogenetic data (geomorphologic and geologic map). A GIS approach was used to integrate each influencing factor which has its own degree of effect. The groundwater recharge potential zone in Yogyakarta is well estimated using this method.',\n",
       " 'The magnetotelluric (MT) 1D inversion modeling is relatively simple and can be considered as a solved problem. Nevertheless, this subject still attracts researchers to study further for its nonlinearity, equivalence and usefulness for particular situation. For example, in the preliminary stage of a geothermal exploration program, 1D MT modeling is still employed to obtain an overall view of the subsurface resistivity distribution in the prospect area. Another example is the regional study of basement structures where the filling sediments are mostly stratified. From the',\n",
       " 'In direct current (DC) sounding (VES) data, inversion technique is applied to obtain the electrical property (resistivity) of subsurface. Inversion of VES data is a non-linear problem. Therefore, non-linear inversion techniques are used in this research, i.e. Particle Swarm Optimization (PSO), Genetic Algorithm (GA), and Dragonfly Algorithm (DA). PSO and GA are frequently used by researchers to solve VES inversion problem. However, DA has never been applied for that problem. These algorithms combine the explorative (global search) and exploitative (local search) concepts to obtain global optimum solution. Initially these algorithms are tested for noise free and added synthetic data in order to assess their ability. In both synthetic data inversion result, these algorithms are able to obtain model parameters which are sufficiently close to true model. In convergence rate, PSO and GA tend to be exploitative (fast convergence rate), while DA tends to be explorative (low convergence rate). In field data, these algorithms are applied to identify the sediment layer in the dried lake area, around lake Ayamaru, Sorong, Papua. In inversion result, the sediment layer has the thickness and resistivity value repectively about 3.3 to 3.5 m of dried lake surface and 20–27 Ωm.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geotexts[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Pre-processing the text\n",
    "\n",
    "The first thing we must do is convert the strings of words into lists of tokens (where a token indicates what is separated by a space). We will use the nltk package to do this. After doing so, we will quickly analyse our corpus and look at different preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join lines together so it becomes one long line\n",
    "text = \" \".join(geotexts)\n",
    "\n",
    "# Separate out the sentences \n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "# Seperate out each word within each sentence\n",
    "tokenised_sents = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yogyakarta',\n",
       " 'is',\n",
       " 'one',\n",
       " 'of',\n",
       " 'most-populated',\n",
       " 'provinces',\n",
       " 'in',\n",
       " 'Indonesia',\n",
       " 'having',\n",
       " 'a',\n",
       " 'high',\n",
       " 'groundwater',\n",
       " 'utilization',\n",
       " '.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us look at our first sentence, now that it has been tokenised\n",
    "tokenised_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: how many tokens do we have in total?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 160321\n"
     ]
    }
   ],
   "source": [
    "total_tokens = [t for sent in tokenised_sents for t in sent]\n",
    "\n",
    "print ('Total number of tokens: %i'%len(total_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do:**\n",
    "1. How many unique tokens do we have?\n",
    "2. Lowercase all the tokens\n",
    "3. Advanced: stem each token (hint: check out https://www.nltk.org/howto/stem.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Compute the word embeddings\n",
    "\n",
    "We are going to use gensim's modelling package: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "\n",
    "In the first example we will use the tokenised sentences with minimal preprocessing and follow the same modelling methodology as was used for the wiki embeddings: \n",
    "- Skipgram approach\n",
    "- Vector size of 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us look at the doc string for the function we will use to create the embeddings\n",
    "gensim.models.Word2Vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Generating skipgram embeddings from our corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27776237, 40080250)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Skip-gram model\n",
    "sg_geoscience = gensim.models.Word2Vec(tokenised_sents, sg=1, min_count=2, window=5, vector_size=300)\n",
    "sg_geoscience.train(tokenised_sents, total_examples=len(tokenised_sents), epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min 54s ± 8.26 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Skip-gram model\n",
    "sg_geoscience = gensim.models.Word2Vec(tokenised_sents, sg=1, min_count=2, window=5, vector_size=300)\n",
    "sg_geoscience.train(tokenised_sents, total_examples=len(tokenised_sents), epochs=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7305"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets access the embeddings and see how many were created?\n",
    "len(sg_geoscience.wv.vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do:**\n",
    "1. Vary the size of the vector, how does this influence training time?\n",
    "2. Change the min_count, how does this influence the number of embeddings created?\n",
    "3. Advanced: create a new set of embeddings from a CBOW methodology (hint: look at the modelling documentation for class gensim.models.word2vec.Word2Vec on https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART THREE: COMPARING THE EMBEDDINGS\n",
    "\n",
    "Using the analysis techniques from Part 1 and the embeddings we created in Part 2. However, this time for our analysis we are going to focus on a goescientific use case. Therefore, all the terms and anologies will be geoscience-related. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: We have created a little function to compare the most similar word to a specified word for both the premade and custom embeddings. In this example we look at the most similar words to the term 'signal'.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparing_embeddings_similarity(word, g_emb, sg_emb):\n",
    "    g    = pd.DataFrame(g_emb.most_similar(positive=[word])[:5],columns=[\"g_name\",\"g_score\"])\n",
    "    sg   = pd.DataFrame(sg_emb.wv.most_similar(positive=[word])[:5],columns=[\"sg_name\",\"sg_score\"])\n",
    "    \n",
    "    df = pd.concat([g, sg],axis = 1)\n",
    "    display (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g_name</th>\n",
       "      <th>g_score</th>\n",
       "      <th>sg_name</th>\n",
       "      <th>sg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shales</td>\n",
       "      <td>0.783693</td>\n",
       "      <td>Barnett</td>\n",
       "      <td>0.380249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shale</td>\n",
       "      <td>0.750160</td>\n",
       "      <td>About</td>\n",
       "      <td>0.367455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oil-shale</td>\n",
       "      <td>0.711005</td>\n",
       "      <td>Umr</td>\n",
       "      <td>0.367139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>siltstone</td>\n",
       "      <td>0.696240</td>\n",
       "      <td>roughly</td>\n",
       "      <td>0.357243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dolostone</td>\n",
       "      <td>0.694625</td>\n",
       "      <td>Fiqa</td>\n",
       "      <td>0.355360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      g_name   g_score  sg_name  sg_score\n",
       "0     shales  0.783693  Barnett  0.380249\n",
       "1      Shale  0.750160    About  0.367455\n",
       "2  oil-shale  0.711005      Umr  0.367139\n",
       "3  siltstone  0.696240  roughly  0.357243\n",
       "4  dolostone  0.694625     Fiqa  0.355360"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word = 'shale' \n",
    "\n",
    "comparing_embeddings_similarity(word, wiki_w2v, sg_geoscience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some terms may not have been available in both corpi and therefore no embedding will exist for that term. In this scenario, our function will not work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Marmousi' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-f6020555f155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Marmousi'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcomparing_embeddings_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki_w2v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg_geoscience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-ad3eb3df5ece>\u001b[0m in \u001b[0;36mcomparing_embeddings_similarity\u001b[0;34m(word, g_emb, sg_emb)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcomparing_embeddings_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mg\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"g_name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"g_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msg\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msg_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sg_name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sg_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlg_summer_school/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    760\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlg_summer_school/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlg_summer_school/lib/python3.9/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'Marmousi' not present\""
     ]
    }
   ],
   "source": [
    "word = 'Marmousi' \n",
    "comparing_embeddings_similarity(word, wiki_w2v, sg_geoscience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can just look in the geoscience corpus at what is the most similar, as opposed to running a comparison of the two embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sg_name</th>\n",
       "      <th>sg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEG/EAGE</td>\n",
       "      <td>0.575661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>implementations</td>\n",
       "      <td>0.559229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Marmousi‐2</td>\n",
       "      <td>0.491406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Foothill</td>\n",
       "      <td>0.485939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non‐quadratic</td>\n",
       "      <td>0.417160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>II</td>\n",
       "      <td>0.413697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pre-Stack</td>\n",
       "      <td>0.404282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>difference-based</td>\n",
       "      <td>0.399245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AzAVO</td>\n",
       "      <td>0.399020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bouguer</td>\n",
       "      <td>0.397072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sg_name  sg_score\n",
       "0          SEG/EAGE  0.575661\n",
       "1   implementations  0.559229\n",
       "2        Marmousi‐2  0.491406\n",
       "3          Foothill  0.485939\n",
       "4     non‐quadratic  0.417160\n",
       "5                II  0.413697\n",
       "6         Pre-Stack  0.404282\n",
       "7  difference-based  0.399245\n",
       "8             AzAVO  0.399020\n",
       "9           Bouguer  0.397072"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'Marmousi' \n",
    "\n",
    "pd.DataFrame(sg_geoscience.wv.most_similar(positive=[word]),columns=[\"sg_name\",\"sg_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do:**\n",
    "1. What is the most similar term to wave?\n",
    "2. What is the most similar term to migration?\n",
    "3. Advanced: Incorporate the custom made CBoW embeddings from Part 2 into the function and rerun the similarity studies.\n",
    "4. Advanced: Add an option into the function to return the least similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sg_name</th>\n",
       "      <th>sg_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>converted-wave</td>\n",
       "      <td>0.439103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>single-sensor</td>\n",
       "      <td>0.427688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>landstreamers</td>\n",
       "      <td>0.427032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S-wave</td>\n",
       "      <td>0.420408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C-wave</td>\n",
       "      <td>0.416430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SH-wave</td>\n",
       "      <td>0.415499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>subtracted</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>assurance</td>\n",
       "      <td>0.362692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sorting</td>\n",
       "      <td>0.354691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>converted</td>\n",
       "      <td>0.346103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          sg_name  sg_score\n",
       "0  converted-wave  0.439103\n",
       "1   single-sensor  0.427688\n",
       "2   landstreamers  0.427032\n",
       "3          S-wave  0.420408\n",
       "4          C-wave  0.416430\n",
       "5         SH-wave  0.415499\n",
       "6      subtracted  0.366667\n",
       "7       assurance  0.362692\n",
       "8         sorting  0.354691\n",
       "9       converted  0.346103"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Let us now consider the word vector maths. In this case let us consider the coal equivalent of a salt dome.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cupola', 0.5283207893371582), ('domes', 0.511414647102356), ('round-topped', 0.49815085530281067), ('roof', 0.4923069179058075), ('domed', 0.48560717701911926), ('smokestack', 0.48251205682754517), ('firebox', 0.4752655625343323), ('trainshed', 0.4744787812232971), ('chimneys', 0.47257155179977417), ('skylight', 0.46635544300079346)]\n",
      "[('seam', 0.40692535042762756), ('inhomogeneities', 0.4021550416946411), ('organic-rich', 0.3903917074203491), ('Selar', 0.3770110607147217), ('Cornish', 0.3614294230937958), ('ash', 0.3448043167591095), ('dolerite', 0.33493053913116455), ('time-step', 0.33342188596725464), ('Ombuku', 0.33070141077041626), ('sides', 0.3279433250427246)]\n"
     ]
    }
   ],
   "source": [
    "print (wiki_vecs.most_similar(positive=[\"dome\", \"coal\"],negative=[\"salt\"]))\n",
    "print (sg_geoscience.wv.most_similar(positive=[\"dome\", \"coal\"],negative=[\"salt\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do:**\n",
    "1. Determine what is the shear equivalent of a P-wave\n",
    "2. Write your own geoscience analogy (see https://www.earthdoc.org/docserver/fulltext/fb/38/7/fb2020051.pdf?expires=1619893516&id=id&accname=fromqa190&checksum=9E55711AF8CF1D67250F04B959D084CD for inspiration)\n",
    "3. Advanced: Incorporate the custom made CBoW embeddings from Part 2 into the function and rerun the analogy studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final task: vary all the hyperparameters of the pipeline for embedding creation (part two) and see how it changes the results in part three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
